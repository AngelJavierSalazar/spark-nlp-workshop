{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "367eba97",
   "metadata": {},
   "source": [
    "# Set some environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "354272e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SECRET=3.0.3-2dab1ba2bf29ada4ba2a708dd1525735e7a91eb1\n",
      "env: SPARK_NLP_LICENSE=eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJleHAiOjE2NTE1ODgwODYsImlhdCI6MTYyMDA1MjA4NiwidW5pcXVlX2lkIjoiYzZhN2NjNzQtYWMxYi0xMWViLWExYTItN2FkZjIzYjkyNjdhIn0.RKMhHuYf80mAjbQNNd6Y8F-tLtzzLCuBecP2wQf07OIbz86dTkTRJt-i6XSzfzxm9onKK_PfP8klbeDAg8z92pKx9on_LFrXETxbCAk9o3I5HEs5d_S_U2sAxyi_zee5CFZ9MYqRtEBm1amRRiFjTrb_F6Pa-mwJIOPItks7bU6dvauRmwuves6YnhzT0ayACYZ-FaMOIEu9F_hgF7ZRQUlAh9Hr_m6Y-Rl-YkC3ksn7hB205ESHzY0ISybTPs6YOlrxgFL5EZclIyWBiClDvoVVxoRLOoKeiHQ5iXMOR1xkVOi6RWtbVPoIlTl8oSG1f2U-f13UwG8Bqs7_R0_6Yg\n",
      "env: AWS_ACCESS_KEY_ID=AKIASRWSDKBGBZYVDUPE\n",
      "env: AWS_SECRET_ACCESS_KEY=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\n",
      "env: JSL_VERSION=3.0.3\n",
      "env: PUBLIC_VERSION=3.0.3\n",
      "env: PYSPARK=3.0.2\n",
      "env: SPARK_HOME=/home/ec2-user/SageMaker/spark-3.0.2-bin-hadoop2.7\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "# Upload your spark_nlp_for_healthcare.json  to the default directory and then run this cell to authenticate \n",
    "with open('spark_nlp_for_healthcare.json', 'r') as f:\n",
    "    for k, v in json.load(f).items():\n",
    "        %set_env $k=$v\n",
    "\n",
    "%set_env PYSPARK=3.0.2\n",
    "%set_env SPARK_HOME=/home/ec2-user/SageMaker/spark-3.0.2-bin-hadoop2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0cd583",
   "metadata": {},
   "source": [
    "# Download packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aa7662e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-01 17:44:11--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/jsl_sagemaker_setup.sh\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1471 (1.4K) [text/plain]\n",
      "Saving to: ‘jsl_sagemaker_setup.sh.2’\n",
      "\n",
      "jsl_sagemaker_setup 100%[===================>]   1.44K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-07-01 17:44:11 (28.1 MB/s) - ‘jsl_sagemaker_setup.sh.2’ saved [1471/1471]\n",
      "\n",
      "3.0.2\n",
      "setup SageMaker for PySpark 3.0.2 and Spark NLP 3.0.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-pyspark 1.4.2 requires pyspark==2.4.0, but you have pyspark 3.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 1 line install for sagemaker\n",
    "!wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/jsl_sagemaker_setup.sh\n",
    "!bash jsl_sagemaker_setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a86bba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sparknlp_jsl\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp_jsl import start\n",
    "from pyspark.ml import PipelineModel\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "# Start authenticated sparksession for NLP-Healthcare\n",
    "spark = start(os.environ[\"SECRET\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48b2cba",
   "metadata": {},
   "source": [
    "# Download a simple model to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45950073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbiobertresolve_rxcui download started this may take some time.\n",
      "Approximate size to download 53.2 MB\n",
      "[OK!]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ENTITY_57d31cc0027b"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentenceEntityResolverModel.pretrained(\"sbiobertresolve_rxcui\" , \"en\", \"clinical/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2621182a",
   "metadata": {},
   "source": [
    "# Download something more interesting (this will require at least an ml.t2.large instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888bade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Healthcare pipe  \n",
    "documentAssembler = DocumentAssembler()\\\n",
    "      .setInputCol(\"text\")\\\n",
    "      .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "sbert_embedder = BertSentenceEmbeddings\\\n",
    "      .pretrained('sbiobert_base_cased_mli', 'en','clinical/models')\\\n",
    "      .setInputCols([\"ner_chunk\"])\\\n",
    "      .setOutputCol(\"sbert_embeddings\")\n",
    "    \n",
    "umls_resolver = SentenceEntityResolverModel.pretrained(\"sbiobertresolve_umls_major_concepts\", \"en\", \"clinical/models\") \\\n",
    "      .setInputCols([\"ner_chunk\", \"sbert_embeddings\"]) \\\n",
    "      .setOutputCol(\"umls_code\")\\\n",
    "      .setDistanceFunction(\"EUCLIDEAN\")\n",
    "\n",
    "umls_pipelineModel = PipelineModel(\n",
    "    stages = [\n",
    "        documentAssembler,\n",
    "        sbert_embedder,\n",
    "        umls_resolver])\n",
    "\n",
    "\n",
    "umls_lp = LightPipeline(umls_pipelineModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "umls_lp\n",
    "text = 'type two diabetes mellitus'\n",
    "umls_lp.annotate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1782ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
