{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize drug names and dosage units with Spark NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we will explain how to use the new Spark NLP's `DrugNormalizer` pretrained models to standardize drug names and dosage units to increase the performance of other NLP pipelines that depends on the identified entities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "source": [
    "Automatic drug normalization is the process of transforming drug names and dosages into a standardized format. For example, it may be important to keep all dosages in the same unit, and drug names without abbreviation. By making the drug-related entities normalized, we can better fill the information into other machine learning pipelines and achieve better results.\n",
    "\n",
    "Examples of normalization:\n",
    "\n",
    "- __Adalimumab 54.5 + 43.2 gm__: The dosage is not standardized and there is a typo\n",
    "- __Agnogenic one half cup__: The dosage is given in natural language (half cup)             \n",
    "- __Interferon alpha-2b 10 million unit (1 ml) injec__: The dosage is a mix of number and text, and there is an abbreviation\n",
    "- __Aspirin 10 meq/ 5 ml oral sol__: The dosage is not in standard unit and we have an abbreviation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To address this problem, many government agencies and private organizations make efforts to create standards and normalization for drug names, description, and dosage to help practitioners to keep information more structured, ease the exchange of information and improve the accessibility to health data. Some of these initiatives are:\n",
    "\n",
    "- [RxNORM](https://www.nlm.nih.gov/research/umls/rxnorm): Provides normalized names for clinical drugs and links its names to many of the drug vocabularies commonly used in pharmacy management and drug interaction software.\n",
    "- [SNOMED CT](https://www.nlm.nih.gov/healthit/snomedct/index.html): The Systematized Nomenclature of Medicine - Clinical Terms, maintained by the not-for-profit association [International Health Terminology Standards Development Organisation](https://www.ihtsdo.org/) (London - UK). It contains clinical terminology for use in U.S. Federal Government systems for the electronic exchange of clinical health information.\n",
    "- [ICD](https://www.who.int/standards/classifications/classification-of-diseases): The International Statistical Classification of Diseases and Related Health Problems (ICD) is a standard for diagnostic classification for both clinical and research purposes endorsed by the World Health Organization (WHO), where the version 10 has been cited more than 20,000 times. ICD defines the universe of diseases, disorders, injuries and other related health conditions, listed in a comprehensive, hierarchical fashion."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "These vocabularies/dictionaries can be explored online and linked to programs through APIs, but some times the search is not optimal due to the complex nature of natural language, and certain ways of writing the search query can help find the correct entry.\n",
    "\n",
    "For this reason, [John Snow Labs](https://www.johnsnowlabs.com) believes it to be a crucial tool to be added on the healthcare solutions the company provides, helping the clients to find the correct references faster and more accurately.    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### John Snow Labs' Spark NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spark NLP](https://nlp.johnsnowlab.com) is an open source Natural Language Processing (NLP) library built on top of Apache Spark from [John Snow Labs](https://www.johnsnowlabs.com), with support for **Python**, **R**, **Scala**, and **Java** programming languages. It is one of the top growing NLP production-ready solution with support for all main tasks related to NLP in more than 46 languages.  \n",
    "\n",
    "\n",
    "<img src=\"images/jsl_sparknlp_summary.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the open souce library, the company maintain the licensed library specialized for healthcare solutions, [SPARK NLP for Healthcare](https://www.johnsnowlabs.com/spark-nlp-health/). According to the [2020 NLP Survey](https://gradientflow.com/2020nlpsurvey/) by Gradient Flow, from the users of NLP solutions in healthcare, Spark NLP accounts for 54%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/jsl_sparknlp_healthcare_summary.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since its [2.7.3 release](https://www.johnsnowlabs.com/spark-nlp-for-healthcare-2-7-3-is-available-now-biobert-based-relation-extraction-models-higher-accuracy-entity-resolution-massive-improvements-in-de-identification-extracting-body-part-relations/),  Spark NLP for Healthcare contains pretrained models for drug names and dosage units normalization, which can improve the performance of tasks that depend on correctly identifying drug entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug Entity Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition is a well known NLP task used to extract useful information from free text. In the healthcare context, useful information may be, for example, drug name, dosage, strength, route, form, and frequency. Given the text:\n",
    "\n",
    "```\n",
    "\" The patient is a 30-year-old female with a long history of insulin dependent diabetes, type 2; coronary artery disease; chronic renal insufficiency; peripheral vascular disease, also secondary to diabetes; who was originally admitted to an outside hospital for what appeared to be acute paraplegia, lower extremities. She did receive a course of Bactrim for 14 days for UTI. \n",
    "Evidently, at some point in time, the patient was noted to develop a pressure-type wound on the sole of her left foot and left great toe. She was also noted to have a large sacral wound; this is in a similar location with her previous laminectomy, and this continues to receive daily care. The patient was transferred secondary to inability to participate in full physical and occupational therapy and continue medical management of her diabetes, the sacral decubitus, left foot pressure wound, and associated complications of diabetes. \n",
    "She is given Fragmin 5000 units subcutaneously daily, Xenaderm to wounds topically b.i.d., Lantus 40 units subcutaneously at bedtime, OxyContin 30 mg p.o. q.12 h., folic acid 1 mg daily, levothyroxine 0.1 mg p.o. daily, Prevacid 30 mg daily, Avandia 4 mg daily, Norvasc 10 mg daily, Lexapro 20 mg daily, aspirin 81 mg daily, Senna 2 tablets p.o. q.a.m., Neurontin 400 mg p.o. t.i.d., Percocet 5/325 mg 2 tablets q.4 h. p.r.n., magnesium citrate 1 bottle p.o. p.r.n., sliding scale coverage insulin, Wellbutrin 100 mg p.o. daily, and Bactrim DS b.i.d.\"\n",
    "```\n",
    "we can identify the following entities (from last paragraph):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/highlighted_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, some of the entities can be abbreviated or given in different units.\n",
    "\n",
    "| Example                                          | Problem(s)                   | Normalized                                        |\n",
    "|--------------------------------------------------|----------------------------|----------------------------------------------------|\n",
    "| Adalimumab 54.5 + 43.2 gm                        | dosage <br/> typo        | Adalimumab **97.7 mg**                                 |\n",
    "| Agnogenic one half cup                           | dosage             | Agnogenic **0.5** cup                                  |\n",
    "| Interferon alpha-2b 10 million unit (1 ml) injec | dosage <br/> abbreviation | Interferon alpha-2b **10000000 unt** (1 ml) **injection** |\n",
    "| Aspirin 10 meq/ 5 ml oral sol                           | dosage <br/> abbreviation             | aspirin **2 meq/ml** oral **solution**                                  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the entities are not normalized as in the examples from the table above, it may be harder for them to be correctly identified  or to be linked to a knowledge base (entity linking, or entity resolve). In the following sections we will exemplify how to use of Spark NLP to create pipelines that improve entity resolution at scale through the new **Drug Normalizer** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark NLP is built on top of Apache Spark and is based in the concept of pipelines, where we can perform several NLP tasks in our data with a unified pipeline. For an introduction of Spark NLP functionalities, refer to \\[1\\].\n",
    "\n",
    "Here, we will explore the Spark NLP for Healthcare capabilities for **Entity Resolution** using the **Drug Normalizer** model. We will first divide our pipeline in two steps for clarification:\n",
    "\n",
    "1. Named Entity Resolution: Identify drug entities in text\n",
    "2. Entity Resolver: Link the identified drug entities to the knowledge bases\n",
    "\n",
    "Then we will show how to use drug normalizer in this pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NER pipeline:\n",
    "\n",
    "```python\n",
    "# Import modules\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from sparknlp.base import DocumentAssembler, LightPipeline\n",
    "from sparknlp.annotator import SentenceDetectorDLModel, Tokenizer, SentenceDetector\n",
    "from sparknlp_jsl.annotator import WordEmbeddingsModel, NerDLModel, NerConverter\n",
    "\n",
    "documentAssembler = DocumentAssembler()\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "  .setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "  .setInputCols([\"sentence\"])\\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "  .setInputCols([\"sentence\", \"token\"])\\\n",
    "  .setOutputCol(\"embeddings\")\n",
    "\n",
    "clinical_ner = NerDLModel.pretrained(\"ner_clinical\", \"en\", \"clinical/models\") \\\n",
    "  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "  .setOutputCol(\"ner\")\n",
    "\n",
    "ner_converter = NerConverter() \\\n",
    "  .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
    "  .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "# NER Pipeline \n",
    "ner_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        documentAssembler,\n",
    "        sentenceDetector,\n",
    "        tokenizer,\n",
    "        word_embeddings,\n",
    "        clinical_ner,\n",
    "        ner_converter])\n",
    "\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
    "ner_model = ner_pipeline.fit(empty_df)\n",
    "```"
   ]
  },
  {
   "source": [
    "With the NER pipeline created, we can identify related entities to be passed on a further pipeline."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entity resolution pipeline built on top of the NER pipeline:\n",
    "\n",
    "```python\n",
    "# Import modules\n",
    "from sparknlp.base import Chunk2Doc, DocumentAssembler\n",
    "from sparknlp_jsl.annotator import BertSentenceEmbeddings\n",
    "from sparknlp_jsl.annotator import SentenceEntityResolverModel\n",
    "\n",
    "c2doc = Chunk2Doc()\\\n",
    "      .setInputCols(\"ner_chunk\")\\\n",
    "      .setOutputCol(\"ner_chunk_doc\")\n",
    "\n",
    "embeddings = BertSentenceEmbeddings\\\n",
    "      .pretrained('sbiobert_base_cased_mli', 'en','clinical/models')\\\n",
    "      .setInputCols([\"ner_chunk\"])\\\n",
    "      .setOutputCol(\"sbert_embeddings\")\n",
    "    \n",
    "resolver = SentenceEntityResolverModel.pretrained(\"sbiobertresolve_rxnorm\", \"en\", \"clinical/models\") \\\n",
    "  .setInputCols([\"ner_chunk\", \"sbert_embeddings\"]) \\\n",
    "  .setOutputCol(\"rxnorm_code\")\\\n",
    "  .setDistanceFunction(\"EUCLIDEAN\")\n",
    "\n",
    "resolver_pipelineModel = PipelineModel(\n",
    "    stages = [\n",
    "        ner_pipeline,\n",
    "        c2doc,\n",
    "        embeddings,\n",
    "        resolver])\n",
    "\n",
    "resolver_lp = LightPipeline(resolver_pipelineModel)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used the stage `Chunk2Doc` because `BertSentenceEmbeddings` input columns should be an annotator of class `Document`, and the previous NER pipeline output was of type `Chunk`. The Entity Resolver pipelines will be described in more details in another post."
   ]
  },
  {
   "source": [
    "## Using the Drug Normilizer stage"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how can we use the drug normalizer in the pipeline? If we are interested in normalizing texts only, we can create a pipeline just for this:\n"
   ]
  },
  {
   "source": [
    "```python\n",
    "document_assembler = DocumentAssembler()\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "drug_normalizer = DrugNormalizer() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"document_normalized\") \\\n",
    "    .setPolicy( \"all\")\n",
    "\n",
    "drug_normalizer_pipeline = \\\n",
    "    Pipeline().setStages([document_assembler, drug_normalizer])\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "dn_model = drug_normalizer_pipeline.fit(empty_df)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new `DrugNormalizer` stage can normalize **Abbreviations**, **Dosages** or both (**all** - default), option that can be set using the `.setPolicy(policy)` where `policy` is one of the possible values: `abbreviations`, `dosage` or `all`, respectively. Additionaly, it can set the option to transform the text to lower case using `.setLowercase(False)`. The default value is `False`. \n",
    "\n",
    "Now, we can use the examples in the introduction to normalize the drug names and dosage units, and then use the entity resolver to search for the entities in the RxNorm data base.\n",
    "\n",
    "First, we create a Spark data frame with the examples and normalize the entities:"
   ]
  },
  {
   "source": [
    "```python\n",
    "# Create the Spark Data Frame\n",
    "example_df = spark.createDataFrame(pd.DataFrame({'text': [\"Adalimumab 54.5 + 43.2 gm\", \n",
    " \"Agnogenic one half cup\", \n",
    " \"Interferon alpha-2b 10 million unit (1 ml) injec\", \n",
    " \"Aspirin 10 meq/ 5 ml oral sol\"]}))\n",
    "\n",
    "# Normalize the entities\n",
    "dn_model.transform(ner_df).show()\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Which gives the following output:\n",
    "\n",
    "| text                 | document             | sentence             | token                | embeddings           | ner                  | ner_chunk            | document_normalized  |\n",
    "|----------------------|----------------------|----------------------|----------------------|----------------------|----------------------|----------------------|----------------------|\n",
    "| Adalimumab 54.5 +... | [[document, 0, 24... | [[document, 0, 24... | [[token, 0, 9, Ad... | [[word_embeddings... | [[named_entity, 0... | [[chunk, 0, 9, Ad... | [[document, 0, 18... |\n",
    "| Agnogenic one hal... | [[document, 0, 21... | [[document, 0, 21... | [[token, 0, 8, Ag... | [[word_embeddings... | [[named_entity, 0... | [[chunk, 0, 21, A... | [[document, 0, 26... |\n",
    "| Interferon alpha-... | [[document, 0, 47... | [[document, 0, 47... | [[token, 0, 9, In... | [[word_embeddings... | [[named_entity, 0... | [[chunk, 0, 18, I... | [[document, 0, 52... |\n",
    "| Aspirin 10 meq/ 5... | [[document, 0, 28... | [[document, 0, 28... | [[token, 0, 6, As... | [[word_embeddings... | [[named_entity, 0... | [[chunk, 0, 6, As... | [[document, 0, 29... |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We then can take the output of the drug normalizer to create a new data frame with the transformed text:\n",
    "\n",
    "```python\n",
    "normalized_df = dn_model.transform(ner_df).select(F.expr(\"document_normalized.result[0]\").alias(\"text\"))\n",
    "normalized_df.show()\n",
    "```\n",
    "Resulting:\n",
    "\n",
    "| text                                                    |\n",
    "|---------------------------------------------------------|\n",
    "| Adalimumab 97700 mg                                     |\n",
    "| Agnogenic 0.5 oral solution                             |\n",
    "| Interferon alpha - 2b 10000000 unt ( 1 ml   ) injection |\n",
    "| Aspirin 2 meq/ml oral solution                          |\n",
    "\n",
    "Then, we can use this data frame as input to the NER pipeline and Entity Resolver as before. As an alternative, we can use the Drug Normalizer in a complete NER pipeline:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```python\n",
    "# NER stages\n",
    "document_assembler = DocumentAssembler()\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "  .setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "  .setInputCols([\"sentence\"])\\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "  .setInputCols([\"sentence\", \"token\"])\\\n",
    "  .setOutputCol(\"embeddings\")\n",
    "\n",
    "clinical_ner = NerDLModel.pretrained(\"ner_posology_large\", \"en\", \"clinical/models\") \\\n",
    "  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "  .setOutputCol(\"ner\")\n",
    "\n",
    "ner_converter = NerConverter() \\\n",
    "  .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
    "  .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "# Convert extracted entities to the doc with chunks in metadata\n",
    "c2doc = Chunk2Doc()\\\n",
    "      .setInputCols(\"ner_chunk\")\\\n",
    "      .setOutputCol(\"ner_chunk_doc\")\n",
    "\n",
    "drug_normalizer = DrugNormalizer() \\\n",
    "    .setInputCols(\"ner_chunk_doc\") \\\n",
    "    .setOutputCol(\"document_normalized\") \\\n",
    "    .setPolicy( \"all\")\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentenceDetector,\n",
    "    tokenizer,\n",
    "    word_embeddings,\n",
    "    clinical_ner,\n",
    "    ner_converter,\n",
    "    c2doc,\n",
    "    drug_normalizer])\n",
    "    \n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "model = nlpPipeline.fit(empty_data)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This complete pipeline can perform all the intermediate tasks in one run with the benefit of being able to do that at scale.  The output is the normalized drug names and dosage units of the identified entities. Now, let's see how to use the drug normalizer before identifying the entities:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```python\n",
    "document_assembler = DocumentAssembler()\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "drug_normalizer = DrugNormalizer() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"document_normalized\") \\\n",
    "    .setPolicy( \"all\")\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "    .setInputCols([\"document_normalized\"])\\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols([\"sentence\"])\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols([\"sentence\", \"token\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "clinical_ner = NerDLModel.pretrained(\"ner_posology_large\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"ner\")\n",
    "\n",
    "ner_converter = NerConverter() \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
    "    .setOutputCol(\"ner_chunk_dn\")\n",
    "\n",
    "dn_nlpPipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    drug_normalizer,\n",
    "    sentenceDetector,\n",
    "    tokenizer,\n",
    "    word_embeddings,\n",
    "    clinical_ner,\n",
    "    ner_converter])\n",
    "    \n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "dn_model = dn_nlpPipeline.fit(empty_data)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We added the `DrugNormalizer` at the beginning of the pipeline. Some times, this allows for better entity recognition, as shown in this examples (also note that in some cases it may be worse, but in general it is better):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"images/drug_with_without_dn.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we introduced a new model to normalize drug names and/or dosage from clinical texts and showed how the new model improves the performance of entity resolvers in the SNOMED knowledge base. You can find an example jupyter notebook at [this Colab notebook](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/23.Drug_Normalizer.ipynb) and if you want to try them on your own data, you can ask for a Spark NLP Healthcare [free trial license](https://www.johnsnowlabs.com/spark-nlp-try-free/).\n",
    "\n",
    "Being used in enterprise projects and built natively on Apache Spark and TensorFlow as well as offering an all-in-one state of the art NLP solutions, Spark NLP library provides simple, performant as well as accurate NLP notations for machine learning pipelines which can scale easily in a distributed environment.\n",
    "\n",
    "If you want to find out more and start practicing Spark NLP, please check out the reference resources below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \\[1\\] [Introduction to spark NLP](https://towardsdatascience.com/introduction-to-spark-nlp-foundations-and-basic-components-part-i-c83b7629ed59) <br/>\n",
    "  \\[2\\] [Text Classification in Spark NLP](https://towardsdatascience.com/text-classification-in-spark-nlp-with-bert-and-universal-sentence-encoders-e644d618ca32) <br/>\n",
    "  \\[3\\] [NER with BERT in Spark NLP](https://towardsdatascience.com/named-entity-recognition-ner-with-bert-in-spark-nlp-874df20d1d77) <br/>\n",
    "  \\[4\\] [John Snow Labs training materials](https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/tutorials/Certification_Trainings) <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.5 64-bit ('dadac': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "6f01399d2ea07497765ba36d63d49a597c9c083d2f3fd2841bddbc8aebc1451c"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}